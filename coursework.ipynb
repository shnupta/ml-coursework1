{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a3d686-9d4f-4755-96fe-51b0600103a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Coursework 1\n",
    "In this assignment, you will implement a decision tree algorithm and use it to determine one of the indoor locations based on WIFI signal strengths collected from a mobile phone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74570545-9f9f-4e19-a2df-afafc43f9cac",
   "metadata": {},
   "source": [
    "## Step 1: Loading data\n",
    "You can load the datasets from the files `wifi_db/clean_dataset.txt` and `wifi_db/noise_dataset.txt`. They contain a 2000x8 array.\n",
    "\n",
    "This array represents a dataset of 2000 samples. Each sample is composed of 7 wifi signal strengths while the last column indicates the room number in which the user is standing (i.e. the label of the sample). **All the features in the dataset are continuous _except_ the room number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a6f15e-d9d9-463d-af41-99cd597bf715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64dbdee0-fb28-483c-a1c3-e36930d6631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the datasets\n",
    "clean_dataset = np.loadtxt('wifi_db/clean_dataset.txt')\n",
    "noisy_dataset = np.loadtxt('wifi_db/noisy_dataset.txt')\n",
    "\n",
    "# print(clean_dataset)\n",
    "# print(noisy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5a712-26be-4ad7-a602-53cf5acdf5bc",
   "metadata": {},
   "source": [
    "## Step 2: Creating Decision Trees\n",
    "To create the decision tree, you will write a recursive function called `decision_tree_learning()`, that takes as arguments a matrix containing the dataset and a depth variable (which is used to compute the maximal depth of the tree, for plotting purposes for instance). The label of the training dataset is assumed to be the last column of the matrix.\n",
    "\n",
    "See the psuedo-code for the algorithm below.\n",
    "```\n",
    "1:  procedure DECISION_TREE_LEARNING(training_dataset, depth)\n",
    "2:    if all samples have the same label then\n",
    "3:      return (a leaf node with this value, depth)\n",
    "4:    else\n",
    "5:      split ← FIND_SPLIT(training dataset)\n",
    "6:      node ← a new decision tree with root as split value\n",
    "7:      l_branch, l_depth ← DECISION_TREE_LEARNING(l_dataset, depth+1)\n",
    "8:      r_branch, r_depth ← DECISION_TREE_LEARNING(r_dataset, depth+1)\n",
    "9:      return (node, max(l_depth, r_depth))\n",
    "10:   end if\n",
    "11: end procedure\n",
    "```\n",
    "\n",
    "The function `FIND_SPLIT` chooses the attribute and the value that results in the highest information gain.\n",
    "\n",
    "An efficient method for finding good split points is to sort the values of the attribute, and then consider only split points that are **between two examples in sorted order**, while keeping track of the running totals of examples of each class for each side of the split point.\n",
    "\n",
    "To evaluate the information gain, suppose that the training dataset S_all has K different labels. We can define two subsets (S_left and S_right) of the dataset depending on the splitting rule and for each dataset and subset, we can compute the distribution (or probability) of each label. For instance, {p1, p2, ..., pk} where pk is the number of samples with the label k divided by the total number of samples from the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea06b80-53be-409a-a553-9dbb1c7cfede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class used to represent decision nodes in a decision tree.\n",
    "\n",
    "Members:\n",
    "- value     : the value on which to split\n",
    "- attribute : the attribute on which to split\n",
    "- left      : the left sub-tree\n",
    "- right     : the right sub-tree\n",
    "\n",
    "Methods:\n",
    "- is_leaf : returns a boolean indicating whether or not this is a leaf node\n",
    "\"\"\"\n",
    "class Node:\n",
    "    def __init__(self, value = None, attribute = None):\n",
    "        self.value = value\n",
    "        self.attribute = attribute\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.label = None\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left == None and self.right == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85add714-c5a9-4b27-bd29-3b043a94e303",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c9/8ftk2vs52fxbn1x0w7kq6yk40000gn/T/ipykernel_60183/2466425547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdecision_tree_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c9/8ftk2vs52fxbn1x0w7kq6yk40000gn/T/ipykernel_60183/2466425547.py\u001b[0m in \u001b[0;36mdecision_tree_learning\u001b[0;34m(training_dataset, depth)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Find the optimum split attribute and value and the corresponding split subsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msplit_attribute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Create a new decision tree with this split value and attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_attribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def find_split(training_dataset):\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "Constructs a decision tree by learning from training_dataset.\n",
    "\n",
    "Arguments:\n",
    "- training_dataset (last column is the label)\n",
    "- depth\n",
    "\n",
    "Returns:\n",
    "- A decision tree in the form of a root Node and subsequent left and right nodes\n",
    "\"\"\"\n",
    "def decision_tree_learning(training_dataset, depth):\n",
    "    unique_labels = np.unique(training_dataset[:,-1])\n",
    "    # If all samples have the same label return a leaf node with this label\n",
    "    if (len(unique_labels) == 1):\n",
    "        node = Node()\n",
    "        node.label = unique_labels[0]\n",
    "        return (node, depth)\n",
    "    else:\n",
    "        # Find the optimum split attribute and value and the corresponding split subsets\n",
    "        split_attribute, split_value, left_dataset, right_dataset = find_split(training_dataset)\n",
    "        # Create a new decision tree with this split value and attribute\n",
    "        node = Node(split_value, split_attribute)\n",
    "        # Construct the rest of the decision tree\n",
    "        left_branch, left_depth = decision_tree_learning(left_dataset, depth + 1)\n",
    "        right_branch, right_depth = decision_tree_learning(right_dataset, depth + 1)\n",
    "        # Assign these branches to the root node\n",
    "        node.left = left_branch\n",
    "        node.right = right_branch\n",
    "        \n",
    "        return (node, max(left_depth, right_depth))\n",
    "\n",
    "decision_tree_learning(clean_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e85b1-5388-45d0-b674-73c39ba12c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
