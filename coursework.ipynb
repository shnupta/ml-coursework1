{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a3d686-9d4f-4755-96fe-51b0600103a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Coursework 1\n",
    "In this assignment, you will implement a decision tree algorithm and use it to determine one of the indoor locations based on WIFI signal strengths collected from a mobile phone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74570545-9f9f-4e19-a2df-afafc43f9cac",
   "metadata": {},
   "source": [
    "## Step 1: Loading data\n",
    "You can load the datasets from the files `wifi_db/clean_dataset.txt` and `wifi_db/noise_dataset.txt`. They contain a 2000x8 array.\n",
    "\n",
    "This array represents a dataset of 2000 samples. Each sample is composed of 7 wifi signal strengths while the last column indicates the room number in which the user is standing (i.e. the label of the sample). **All the features in the dataset are continuous _except_ the room number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a6f15e-d9d9-463d-af41-99cd597bf715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64dbdee0-fb28-483c-a1c3-e36930d6631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-64. -56. -61. ... -82. -81.   1.]\n",
      " [-68. -57. -61. ... -85. -85.   1.]\n",
      " [-63. -60. -60. ... -85. -84.   1.]\n",
      " ...\n",
      " [-62. -59. -46. ... -87. -88.   4.]\n",
      " [-62. -58. -52. ... -90. -85.   4.]\n",
      " [-59. -50. -45. ... -88. -87.   4.]]\n",
      "[[-59. -53. -51. ... -79. -87.   4.]\n",
      " [-66. -53. -59. ... -81. -79.   1.]\n",
      " [-41. -57. -63. ... -66. -65.   2.]\n",
      " ...\n",
      " [-57. -54. -56. ... -79. -82.   1.]\n",
      " [-56. -52. -50. ... -85. -88.   3.]\n",
      " [-46. -54. -47. ... -80. -73.   3.]]\n"
     ]
    }
   ],
   "source": [
    "# Load in the datasets\n",
    "clean_dataset = np.loadtxt('wifi_db/clean_dataset.txt')\n",
    "noisy_dataset = np.loadtxt('wifi_db/noisy_dataset.txt')\n",
    "\n",
    "print(clean_dataset)\n",
    "print(noisy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5a712-26be-4ad7-a602-53cf5acdf5bc",
   "metadata": {},
   "source": [
    "## Step 2: Creating Decision Trees\n",
    "To create the decision tree, you will write a recursive function called `decision_tree_learning()`, that takes as arguments a matrix containing the dataset and a depth variable (which is used to compute the maximal depth of the tree, for plotting purposes for instance). The label of the training dataset is assumed to be the last column of the matrix.\n",
    "\n",
    "See the psuedo-code for the algorithm below.\n",
    "```\n",
    "1:  procedure DECISION_TREE_LEARNING(training_dataset, depth)\n",
    "2:    if all samples have the same label then\n",
    "3:      return (a leaf node with this value, depth)\n",
    "4:    else\n",
    "5:      split ← FIND_SPLIT(training dataset)\n",
    "6:      node ← a new decision tree with root as split value\n",
    "7:      l_branch, l_depth ← DECISION_TREE_LEARNING(l_dataset, depth+1)\n",
    "8:      r_branch, r_depth ← DECISION_TREE_LEARNING(r_dataset, depth+1)\n",
    "9:      return (node, max(l_depth, r_depth))\n",
    "10:   end if\n",
    "11: end procedure\n",
    "```\n",
    "\n",
    "The function `FIND_SPLIT` chooses the attribute and the value that results in the highest information gain.\n",
    "\n",
    "An efficient method for finding good split points is to sort the values of the attribute, and then consider only split points that are **between two examples in sorted order**, while keeping track of the running totals of examples of each class for each side of the split point.\n",
    "\n",
    "To evaluate the information gain, suppose that the training dataset S_all has K different labels. We can define two subsets (S_left and S_right) of the dataset depending on the splitting rule and for each dataset and subset, we can compute the distribution (or probability) of each label. For instance, {p1, p2, ..., pk} where pk is the number of samples with the label k divided by the total number of samples from the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea06b80-53be-409a-a553-9dbb1c7cfede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class used to represent decision nodes in a decision tree.\n",
    "\n",
    "Members:\n",
    "- value     : the value on which to split\n",
    "- attribute : the attribute on which to split\n",
    "- left      : the left sub-tree\n",
    "- right     : the right sub-tree\n",
    "\n",
    "Methods:\n",
    "- is_leaf : returns a boolean indicating whether or not this is a leaf node\n",
    "\"\"\"\n",
    "class Node:\n",
    "    def __init__(self, value = None, attribute = None):\n",
    "        self.value = value\n",
    "        self.attribute = attribute\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.label = None\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left == None and self.right == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chooses the attribute and the value that results in the highest information gain\n",
    "\n",
    "Arguments:\n",
    "- training_dataset (last column is the label)\n",
    "\n",
    "Returns:\n",
    "- tuple : Returns a tuple of (split_attribute, split_value, left_dataset, right_dataset)\n",
    "          - split_attribute: The column index of the value that was split on\n",
    "          - split_value: The value used to split the dataset into left and right\n",
    "          - left_dataset: All elements have a `split_attribute` value < `split_value`\n",
    "          - right_dataset: All elements have a `split_attribute` value >= `split_value`\n",
    "\"\"\"\n",
    "def find_split(training_dataset):\n",
    "    best_split = (0, None)\n",
    "\n",
    "    for attribute in range(training_dataset.shape[1] - 1):\n",
    "        # Sort the rows by the current attribute's values\n",
    "        sorted_attribute_indices = np.argsort(training_dataset, axis=0)[:, attribute]\n",
    "        sorted_by_attribute = training_dataset[sorted_attribute_indices, :]\n",
    "        # Get the index of each point where the value changes\n",
    "        _, split_points = np.unique(sorted_by_attribute[:, attribute], return_index=True)\n",
    "        for split_point in split_points:\n",
    "            # Find the information gained from splitting at this point\n",
    "            left = sorted_by_attribute[:split_point, :]\n",
    "            right = sorted_by_attribute[split_point:, :]\n",
    "            info_gain = information_gain(training_dataset, left, right)\n",
    "            split_value = sorted_by_attribute[split_point, attribute]\n",
    "            # Replace best_split if more information is gained by splitting here\n",
    "            best_split = max(best_split, (info_gain, (attribute, split_value, left, right)), key=lambda x: x[0])\n",
    "\n",
    "    return best_split[1]\n",
    "\n",
    "# Returns the information gain when the dataset `data` is split into two distinct subsets `left` and `right`\n",
    "def information_gain(data, left, right):\n",
    "    return entropy(data) - remainder(left, right)\n",
    "\n",
    "# Returns the information entropy of `dataset`\n",
    "def entropy(dataset):\n",
    "    _, counts = np.unique(dataset[:, -1], return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    return -(probabilities * np.log2(probabilities)).sum()\n",
    "\n",
    "# Returns the (weighted) average entropy of the two subsets `left` and `right`\n",
    "def remainder(left, right):\n",
    "    proportion_left = len(left) / (len(left) + len(right))\n",
    "    return proportion_left * entropy(left) + (1 - proportion_left) * entropy(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85add714-c5a9-4b27-bd29-3b043a94e303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.Node at 0x7fb6ef4b1460>, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Constructs a decision tree by learning from training_dataset.\n",
    "\n",
    "Arguments:\n",
    "- training_dataset (last column is the label)\n",
    "- depth\n",
    "\n",
    "Returns:\n",
    "- A decision tree in the form of a root Node and subsequent left and right nodes\n",
    "\"\"\"\n",
    "def decision_tree_learning(training_dataset, depth):\n",
    "    unique_labels = np.unique(training_dataset[:,-1])\n",
    "    # If all samples have the same label return a leaf node with this label\n",
    "    if (len(unique_labels) == 1):\n",
    "        node = Node()\n",
    "        node.label = unique_labels[0]\n",
    "        return (node, depth)\n",
    "    else:\n",
    "        # Find the optimum split attribute and value and the corresponding split subsets\n",
    "        split_attribute, split_value, left_dataset, right_dataset = find_split(training_dataset)\n",
    "        # Create a new decision tree with this split value and attribute\n",
    "        node = Node(split_value, split_attribute)\n",
    "        # Construct the rest of the decision tree\n",
    "        left_branch, left_depth = decision_tree_learning(left_dataset, depth + 1)\n",
    "        right_branch, right_depth = decision_tree_learning(right_dataset, depth + 1)\n",
    "        # Assign these branches to the root node\n",
    "        node.left = left_branch\n",
    "        node.right = right_branch\n",
    "        \n",
    "        return (node, max(left_depth, right_depth))\n",
    "\n",
    "decision_tree_learning(clean_dataset, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e85b1-5388-45d0-b674-73c39ba12c4a",
   "metadata": {},
   "source": [
    "## Step 3: Evaluation\n",
    "Evaluate your decision tree using a 10-fold cross validation on both the clean and noisy datasets. You should expect that slightly different trees will be created with each fold, since the training data that you use each time will be slightly different. Use your resulting decision trees to classify your data in your test sets.\n",
    "\n",
    "Implement an evaluation function that takes a trained tree and a test dataset: `evaluate(test_db, trained_tree)` and that returns the accuracy of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluates the performance of trained_tree on test_db\n",
    "\n",
    "Arguments:\n",
    "- test_db      : data used for testing (last column is label)\n",
    "- trained_tree : trained decision tree\n",
    "\n",
    "Returns:\n",
    "- accuracy (float)\n",
    "\"\"\"\n",
    "def evaluate(test_db, trained_tree):\n",
    "    return 0.0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
